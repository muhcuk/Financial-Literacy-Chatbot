# -*- coding: utf-8 -*-
"""phi-2_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iDLOj9IIsrZu6fkMI9xzD-1xQOZLsd_7
"""

# Install required packages
print("ğŸ“¦ Installing packages...")
!pip install -q accelerate peft bitsandbytes transformers trl

print("\nâœ… Installation complete!")

# Check GPU
import torch

print("\n" + "="*50)
print("ğŸ–¥ï¸  GPU INFORMATION")
print("="*50)

if torch.cuda.is_available():
    print(f"âœ… GPU Available: YES")
    print(f"ğŸ“› GPU Name: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    print(f"ğŸ”¢ CUDA Version: {torch.version.cuda}")
else:
    print("âŒ No GPU detected!")
    print("âš ï¸  Please enable GPU: Runtime â†’ Change runtime type â†’ T4 GPU")

print("="*50)

# Mount Google Drive
from google.colab import drive
print("ğŸ“ Mounting Google Drive...")
drive.mount('/content/drive')

# Load dataset
from datasets import load_dataset
import os

# Update this path to match where your file is stored
file_path = '/content/drive/MyDrive/phi_finetune/finlit_llama_chat_1000_v2.jsonl'

# Check if file exists
if not os.path.exists(file_path):
    print(f"\nâŒ File not found at: {file_path}")
    print("\nğŸ“‚ Listing files in MyDrive:")
    !ls "/content/drive/MyDrive/"
    print("\nâš ï¸  Please update the file_path variable above")
else:
    print(f"âœ… File found: {file_path}")
    print(f"ğŸ“Š File size: {os.path.getsize(file_path) / 1024:.2f} KB")

    # Load dataset
    print("\nğŸ“¥ Loading dataset...")
    dataset = load_dataset('json', data_files=file_path)

    print(f"\nâœ… Dataset loaded!")
    print(f"ğŸ“ Total samples: {len(dataset['train'])}")
    print(f"\nğŸ” Sample entry:")
    print(dataset['train'][0])

# Format dataset for Phi-2
def format_phi2_prompt(example):
    """Format for Phi-2 chat format"""
    messages = example['messages']

    formatted_text = ""
    for msg in messages:
        role = msg['role']
        content = msg['content']

        if role == 'user':
            formatted_text += f"User: {content}\n\n"
        elif role == 'assistant':
            formatted_text += f"Assistant: {content}\n\n"

    return {'text': formatted_text}

print("Formatting dataset for Phi-2...")
dataset = dataset.map(format_phi2_prompt)

# Split into train/validation (90/10)
dataset = dataset['train'].train_test_split(test_size=0.1, seed=42)

print(f"\nTrain samples: {len(dataset['train'])}")
print(f"Validation samples: {len(dataset['test'])}")

# Show formatted example
print(f"\nFormatted example:")
print("="*80)
print(dataset['train'][0]['text'][:500])
print("...")
print("="*80)

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

model_name = "microsoft/phi-2"
print(f"Model: {model_name}\n")

# 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

# Load model
print("Loading Phi-2 model (this takes 2-3 minutes)...")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
)

print("Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# Prepare for training
print("Preparing model for training...")
model = prepare_model_for_kbit_training(model)

# LoRA configuration
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["Wqkv", "fc1", "fc2"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Apply LoRA
model = get_peft_model(model, lora_config)

print("\nModel ready!")
model.print_trainable_parameters()

import shutil
import os

print("ğŸ§¹ Cleaning up storage...")

# Clear HuggingFace cache
cache_dir = "/root/.cache/huggingface"
if os.path.exists(cache_dir):
    shutil.rmtree(cache_dir)
    print(f"âœ… Cleared: {cache_dir}")

# Clear tmp files
os.system("rm -rf /tmp/*")
print("âœ… Cleared /tmp/")

# Check available space
disk_usage = shutil.disk_usage("/")
print(f"\nğŸ’¾ Available space: {disk_usage.free / 1e9:.2f} GB")

from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

print("âš™ï¸  Setting up training configuration...")

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=512,
        padding="max_length",
    )

print("Tokenizing dataset...")
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=dataset["train"].column_names,
)

# Data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

# Training arguments
training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/llama_finetune/phi2_finlit_checkpoints",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    optim="paged_adamw_8bit",
    logging_steps=10,
    save_strategy="steps",
    save_steps=100,
    eval_strategy="steps",
    eval_steps=100,
    learning_rate=2e-4,
    weight_decay=0.01,
    fp16=False,
    bf16=True,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="cosine",
    report_to="none",
    save_total_limit=3,
)

# Initialize trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset['train'],
    eval_dataset=tokenized_dataset['test'],
    data_collator=data_collator,
)

print("âœ… Trainer configured!")
print("\nğŸ“Š Training Configuration:")
print(f"   - Epochs: {training_args.num_train_epochs}")
print(f"   - Batch size: {training_args.per_device_train_batch_size}")
print(f"   - Gradient accumulation: {training_args.gradient_accumulation_steps}")
print(f"   - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
print(f"   - Learning rate: {training_args.learning_rate}")
print(f"   - Max sequence length: 512")

import time

print("="*80)
print("ğŸš€ STARTING TRAINING")
print("="*80)
print("\nâ° This will take approximately 30-60 minutes depending on your GPU")
print("ğŸ“Š Training progress will be shown below\n")

start_time = time.time()

# Train the model
trainer.train()

end_time = time.time()
training_duration = (end_time - start_time) / 60

print("\n" + "="*80)
print("âœ… TRAINING COMPLETE!")
print("="*80)
print(f"â±ï¸  Total training time: {training_duration:.2f} minutes")

# Save the final model
final_model_path = "/content/drive/MyDrive/llama_finetune/phi2_finlit_final"
print(f"\nğŸ’¾ Saving final model to: {final_model_path}")

trainer.model.save_pretrained(final_model_path)
tokenizer.save_pretrained(final_model_path)

print("âœ… Model saved successfully!")

print("="*80)
print("ğŸ“Š EVALUATING MODEL")
print("="*80)

# Run evaluation
eval_results = trainer.evaluate()

print("\nğŸ“ˆ Evaluation Results:")
print("-" * 40)
for key, value in eval_results.items():
    print(f"{key}: {value:.4f}")

print("\n" + "="*80)

print("="*80)
print("ğŸ§ª TESTING FINE-TUNED MODEL")
print("="*80)

def generate_response(prompt, max_length=256):
    """Generate response using the fine-tuned model"""
    formatted_prompt = f"User: {prompt}\n\nAssistant:"

    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=max_length,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extract only the assistant's response
    if "Assistant:" in response:
        response = response.split("Assistant:")[-1].strip()

    return response

# Test with sample questions
test_prompts = [
    "What is compound interest?",
    "How should I start investing as a beginner?",
    "Explain the difference between stocks and bonds.",
    "What is an emergency fund and why do I need one?",
]

print("\nğŸ” Testing with sample questions:\n")

for i, prompt in enumerate(test_prompts, 1):
    print(f"\n{'='*80}")
    print(f"Question {i}: {prompt}")
    print("-" * 80)

    response = generate_response(prompt)
    print(f"Response: {response}")

print("\n" + "="*80)
print("âœ… Testing complete!")
print("\nğŸ’¡ You can now use the generate_response() function to test custom prompts")
print("Example: generate_response('Your question here')")